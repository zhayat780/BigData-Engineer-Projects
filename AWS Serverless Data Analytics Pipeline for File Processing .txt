s3 Bucket:
-----------
Bucket Name:sftpusecasetest
Folder:landing
       curated
       publish

SFTP:
--------
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Action": [
                "s3:ListBucket",
                "s3:GetBucketLocation"
            ],
            "Resource": [
                "arn:aws:s3:::{s3 bucket name}"
            ],
            "Effect": "Allow",
            "Sid": "ReadWriteS3"
        },
        {
            "Action": [
                "s3:PutObject",
                "s3:GetObject",
                "s3:DeleteObject",
                "s3:DeleteObjectVersion",
                "s3:GetObjectVersion",
                "s3:GetObjectACL",
                "s3:PutObjectACL"
            ],
            "Resource": [
                "arn:aws:s3:::{s3 bucket name}/*"
            ],
            "Effect": "Allow",
            "Sid": ""
        }
    ]
}


pwd
ssh-keygen -t rsa

Test once setup is done 

Lambda Code:
--------------
import boto3
from zipfile import ZipFile

import os
import json
import uuid
myuuid = str(uuid.uuid4()).replace('-','')
glue_job_name='testsftpcsvtoparquet'
def lambda_handler(event, context):
    # TODO implement
    for record in event['Records']:
        file_name_with_directory = record['s3']['object']['key']
        file_name = record['s3']['object']['key'].split('/')[0]
        bucketName=record['s3']['bucket']['name']
        print("File Name : ",file_name)
        print("File Name with directory: ",file_name_with_directory)
        print("Bucket Name : ",bucketName)
        local_file_full_name='/tmp/{}'.format(file_name)
        s3 = boto3.client('s3')
        s3.download_file(bucketName, file_name_with_directory, local_file_full_name)
        print("File downloaded successfully")
        
        with ZipFile(local_file_full_name, 'r') as f:
            #extract in current directory
            f.extractall('/tmp/unzip{}'.format(myuuid))
        file_names=''
        for filename in os.listdir('/tmp/unzip{}'.format(myuuid)):
            f = os.path.join('/tmp/unzip{}'.format(myuuid), filename)
            print("File Name : ",f)
            s3.upload_file(f, bucketName, 'curated/{}'.format(filename))
            os.remove(f)
            file_names=file_names+','+'s3://{}/curated/{}'.format(bucketName,filename)
        print("Files after unzip :", file_names)
        glue=boto3.client('glue');
        response = glue.start_job_run(JobName = glue_job_name, Arguments={"--VAL1":file_names[1:]})
        print("Glue Job trigger response : ",response)
        return {
            'statusCode': 200,
            'body': json.dumps('Hello from Lambda!')
        }

Increase the timeout for Lambda , provide proper permission to the Lambda Role


Glue Code:
------------
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()


def main():
    ## @params: [JOB_NAME]
    args = getResolvedOptions(sys.argv, ["VAL1"])
    file_names=args['VAL1'].split(',')
    df = spark.read.csv(file_names, header = True)
    df.repartition(1).write.mode('append').parquet("s3a://sftpusecasetest/publish/")

main()


Snowflake Code:
---------------------

--drop database if exists
drop database if exists s3_to_snowflake;

--Database Creation 
create database if not exists s3_to_snowflake;

--Use the database
use s3_to_snowflake;

--Table Creation
create or replace table s3_to_snowflake.PUBLIC.Iris_dataset1 (Id number(10,0),sepal_length number(10,4),sepal_width number(10,4),petal_length number(10,4)  ,petal_width number(10,4),class varchar(200));
                                  


--create the file format
CREATE OR REPLACE FILE FORMAT sf_tut_parquet_format
  TYPE = parquet;

--create the external stage
create or replace stage s3_to_snowflake.PUBLIC.Snow_stage url="s3://sftpusecasetest/publish/" 
credentials=(aws_key_id=''
aws_secret_key='')
file_format = sf_tut_parquet_format;

list @Snow_stage;



--Create the Pipe
create or replace pipe s3_to_snowflake.PUBLIC.for_iris_one
auto_ingest=true as 
copy into s3_to_snowflake.PUBLIC.Iris_dataset1
from 
(select $1:"Id"::number,
$1:SEPAL_LENGTH::VARCHAR,
$1:SEPAL_WIDTH::VARCHAR,
$1:PETAL_LENGTH::VARCHAR,
$1:PETAL_WIDTH::VARCHAR,
$1:CLASS_NAME::VARCHAR
from @s3_to_snowflake.PUBLIC.Snow_stage);


show pipes;

select * from s3_to_snowflake.PUBLIC.Iris_dataset1;
